#!/bin/bash

set -e

HERE=$(pwd)
RUN_DIR="${HERE}/$(hostname -f)"

# get arguments
phys_cores=$1
shift 1 || { echo "Must supply number of physical cores" 1>&2; exit 1; }
log_cores=$1
shift 1 || { echo "Must supply number of logical cores" 1>&2; exit 1; }

echo -e "RUNNING BENCHMARK WITH ${phys_cores} PHYSICAL CORES AND ${log_cores} VIRTUAL CORES\n"

# system details
mkdir -p system_details
lscpu &> system_details/cpu.dat
head -n1 /proc/meminfo &> system_details/memsize.dat
ls /dev/disk/by-id | grep -v -- -part &> system_details/disks.dat
cat /etc/os-release &> system_details/os.dat
uname -rm &> system_details/kernel.dat

# -- setup --------------------------------------------------------------------

# -- software setup

echo -e "Setting up software environment\n"
CONDA=$(which mamba || which conda)
#export PREFIX="/var/tmp/$(whoami)/benchmark-env"
export PREFIX="${HERE}/benchmark-env"
if [ ! -f ${PREFIX}/conda-meta/history ]; then
	${CONDA} create \
		--override-channels \
		--channel conda-forge \
		--prefix ${PREFIX} \
		--quiet \
		--yes \
		python=3.9 \
		lalsuite=7.3 \
		mpi4py \
		openmpi \
		pycbc=1.18.3 \
	&> ${HERE}/conda-create.log
fi

# -- download data

DATA_DIR=${HERE}/data
pushd ${DATA_DIR}

# download SEOBNR ROM
if ! (echo "256f33aecada770b871750227fa87749  SEOBNRv4ROM_v2.0.hdf5" | md5sum -c --status -); then
	curl -LO \
		https://git.ligo.org/lscsoft/lalsuite-extra/-/raw/master/data/lalsimulation/SEOBNRv4ROM_v2.0.hdf5 \
		2> log/curl-seobnr-rom.log
fi
export LAL_DATA_PATH="$(pwd)"

# download template bank (XML)
if ! (echo "4fa39669827b7f181b1b7359a8088ce5  H1L1-HYPERBANK_SEOBNRv4v2_VARFLOW_THORNE-1163174417-604800.xml.gz" | md5sum -c --status -); then
	curl -LO \
		https://raw.githubusercontent.com/ligo-cbc/pycbc-config/master/O2/bank/H1L1-HYPERBANK_SEOBNRv4v2_VARFLOW_THORNE-1163174417-604800.xml.gz \
		2> log/curl-bank.log
fi

# download input data
if ! (echo "869f6ae617520269399cb5e3a271ebd9  L-L1_LOSC_4_V1-1126256640-4096.gwf" | md5sum -c --status -); then
	curl -LO \
		https://losc.ligo.org/archive/data/O1/1126170624/L-L1_LOSC_4_V1-1126256640-4096.gwf \
		2> log/curl-losc-data.log
fi

popd

# -- pycbc benchmark

echo "Setting up PyCBC benchmark"
echo

mkdir -p ${RUN_DIR}/pycbc_benchmark/log
pushd ${RUN_DIR}/pycbc_benchmark

# convert template bank to HDF5
rm -rf H1L1-HYPERBANK_SEOBNRv4v2_VARFLOW_THORNE-1163174417-604800.hdf
${PREFIX}/bin/pycbc_coinc_bank2hdf \
	--bank-file ${DATA_DIR}/H1L1-HYPERBANK_SEOBNRv4v2_VARFLOW_THORNE-1163174417-604800.xml.gz \
	--output-file H1L1-HYPERBANK_SEOBNRv4v2_VARFLOW_THORNE-1163174417-604800.hdf \
	&> log/bank2hdf.log
export SCHEME=cpu

# Use this if running the production benchmark
export NTEMPLATES=15000
# Use this if just wanting to test the benchmark
#export NTEMPLATES=100

# Run this after setting NTEMPLATES. If NTEMPLATES changes this must be rerun.
rm -f H1L1-SPLITBANK_*.hdf
${PREFIX}/bin/pycbc_hdf5_splitbank \
	--bank-file H1L1-HYPERBANK_SEOBNRv4v2_VARFLOW_THORNE-1163174417-604800.hdf \
	--templates-per-bank ${NTEMPLATES} \
	--output-prefix H1L1-SPLITBANK_ \
	--random-sort \
	&> log/splitbank.log
mv H1L1-SPLITBANK_0.hdf tmp.hdf
rm -f H1L1-SPLITBANK_*hdf
mv tmp.hdf H1L1-SPLITBANK_0.hdf

export N_JOBS=${log_cores}

popd

# -- waveform benchmark

echo "Setting up waveform benchmark"
echo

mkdir -p ${RUN_DIR}/waveform_benchmark
pushd ${RUN_DIR}/waveform_benchmark
mkdir -p log

if [ ! -d ${DATA_DIR}/TD-wf-bench ]; then
	pushd ${DATA_DIR}
	git clone https://git.ligo.org/duncanmmacleod/TD-wf-bench.git --branch python3 2> log/gitclone.log
	popd
fi

popd

# -- lalinference benchmark

echo "Setting up lalinference benchmark"
echo

mkdir -p ${RUN_DIR}/lalinference_benchmark
pushd ${RUN_DIR}/lalinference_benchmark

# Generate an injection file
${PREFIX}/bin/lalapps_inspinj \
	--gps-start-time 441417609 \
	--gps-end-time 441417639 \
	--m-distr componentMass \
	--min-mass1 5 \
	--min-mass2 5 \
	--max-mass1 5 \
	--max-mass2 5 \
	--max-mtotal 10 \
	--i-distr uniform \
	--waveform IMRPhenomPv2pseudoFourPN \
	--amp-order 0 \
	--l-distr random \
	--f-lower 20 \
	--t-distr uniform \
	--time-step 30 \
	--disable-spin \
	--o injections.xml \
	--snr-distr volume \
	--ifos H1,L1,V1 \
	--ligo-fake-psd LALAdLIGO \
	--virgo-fake-psd LALAdVirgo \
	--min-snr 20 \
	--max-snr 20 \
	--ligo-start-freq 20 \
	--virgo-start-freq 20 \
;

#create directory needed to use the ROQ data and prepare necessary files
mkdir -p ROQdata
mkdir -p log
popd

# -- run ----------------------------------------------------------------------

export OMP_NUM_THREADS=1

# -- PyCBC

echo "Running PyCBC benchmarks"
echo

pushd ${RUN_DIR}/pycbc_benchmark

echo "Test 1 on ${log_cores} cores"
echo
for IDX in $(seq 1 ${N_JOBS}); do
${PREFIX}/bin/pycbc_inspiral \
	--sgchisq-snr-threshold 6.0 \
	--sgchisq-locations "mtotal>40:20-30,20-45,20-60,20-75,20-90,20-105,20-120" \
	--pad-data 8 \
	--strain-high-pass 15 \
	--sample-rate 2048 \
	--segment-length 512 \
	--segment-start-pad 144 \
	--segment-end-pad 16 \
	--allow-zero-padding \
	--taper-data 1 \
	--psd-estimation median \
	--psd-segment-length 16 \
	--psd-segment-stride 8 \
	--psd-inverse-length 16 \
	--psd-num-segments 63 \
	--autogating-threshold 100 \
	--autogating-cluster 0.5 \
	--autogating-width 0.25 \
	--autogating-taper 0.25 \
	--autogating-pad 16 \
	--enable-bank-start-frequency  \
	--low-frequency-cutoff 20 \
	--approximant 'SPAtmplt:mtotal<4' 'SEOBNRv4_ROM:else' \
	--order -1 \
	--snr-threshold 5.5 \
	--cluster-method window \
	--cluster-window 1 \
	--cluster-function symmetric \
	--chisq-bins "0.72*get_freq('fSEOBNRv4Peak',params.mass1,params.mass2,params.spin1z,params.spin2z)**0.7" \
	--newsnr-threshold 5 \
	--filter-inj-only  \
	--injection-window 4.5 \
	--processing-scheme ${SCHEME} \
	--injection-filter-rejector-chirp-time-window 5 \
	--channel-name L1:GDS-CALIB_STRAIN \
	--gps-start-time 1126258462 \
	--gps-end-time 1126260462 \
	--trig-start-time 1126258700 \
	--trig-end-time 1126260000 \
	--output OUTPUTSEOB_${IDX}.hdf \
	--bank-file H1L1-SPLITBANK_0.hdf \
	--frame-files ${DATA_DIR}/L-L1_LOSC_4_V1-1126256640-4096.gwf \
	--channel-name L1:LOSC-STRAIN \
	&> log/test1_${IDX}.log &
done

wait

echo "Test 2 on ${log_cores} cores"
echo
for IDX in $(seq 1 ${N_JOBS}); do
${PREFIX}/bin/pycbc_inspiral \
	--sgchisq-snr-threshold 6.0 \
	--sgchisq-locations "mtotal>40:20-30,20-45,20-60,20-75,20-90,20-105,20-120" \
	--pad-data 8 \
	--strain-high-pass 15 \
	--sample-rate 2048 \
	--segment-length 512 \
	--segment-start-pad 144 \
	--segment-end-pad 16 \
	--allow-zero-padding \
	--taper-data 1 \
	--psd-estimation median \
	--psd-segment-length 16 \
	--psd-segment-stride 8 \
	--psd-inverse-length 16 \
	--psd-num-segments 63 \
	--autogating-threshold 100 \
	--autogating-cluster 0.5 \
	--autogating-width 0.25 \
	--autogating-taper 0.25 \
	--autogating-pad 16 \
	--enable-bank-start-frequency  \
	--low-frequency-cutoff 20 \
	--approximant 'SPAtmplt:mtotal<50' 'SEOBNRv4_ROM:else' \
	--order -1 \
	--snr-threshold 5.5 \
	--cluster-method window \
	--cluster-window 1 \
	--cluster-function symmetric \
	--chisq-bins "0.72*get_freq('fSEOBNRv4Peak',params.mass1,params.mass2,params.spin1z,params.spin2z)**0.7" \
	--newsnr-threshold 5 \
	--filter-inj-only  \
	--injection-window 4.5 \
	--processing-scheme ${SCHEME} \
	--injection-filter-rejector-chirp-time-window 5 \
	--channel-name L1:GDS-CALIB_STRAIN \
	--gps-start-time 1126258462 \
	--gps-end-time 1126260462 \
	--trig-start-time 1126258700 \
	--trig-end-time 1126260000 \
	--output OUTPUTF2_${IDX}.hdf \
	--bank-file H1L1-SPLITBANK_0.hdf \
	--frame-files ${DATA_DIR}/L-L1_LOSC_4_V1-1126256640-4096.gwf \
	--channel-name L1:LOSC-STRAIN \
	&> log/test2_${IDX}.log &
done

wait

${PREFIX}/bin/python - <<EOF
from pathlib import Path
import h5py

for tag, outf in (
    ('OUTPUTSEOB', 'w_hp_perf_num_1.dat'),
    ('OUTPUTF2', 'w_hp_perf_num_2.dat'),
):
    filelist = list(Path('.').glob(f'{tag}_*.hdf'))
    assert len(filelist) == ${N_JOBS}, f'wrong number of files for {tag}'

    perf_tot = 0
    for f in filelist:
        with h5py.File(f, 'r') as h5f:
            perf_tot += 1. / h5f['L1/search/run_time'][0]

    Path(outf).write_text(str(perf_tot))
EOF

export N_JOBS=${phys_cores}

rm -f *OUTPUT*

echo "Test 1 on ${phys_cores} cores"
echo
for IDX in $(seq 1 ${N_JOBS}); do
${PREFIX}/bin/pycbc_inspiral \
	--sgchisq-snr-threshold 6.0 \
	--sgchisq-locations "mtotal>40:20-30,20-45,20-60,20-75,20-90,20-105,20-120" \
	--pad-data 8 \
	--strain-high-pass 15 \
	--sample-rate 2048 \
	--segment-length 512 \
	--segment-start-pad 144 \
	--segment-end-pad 16 \
	--allow-zero-padding \
	--taper-data 1 \
	--psd-estimation median \
	--psd-segment-length 16 \
	--psd-segment-stride 8 \
	--psd-inverse-length 16 \
	--psd-num-segments 63 \
	--autogating-threshold 100 \
	--autogating-cluster 0.5 \
	--autogating-width 0.25 \
	--autogating-taper 0.25 \
	--autogating-pad 16 \
	--enable-bank-start-frequency  \
	--low-frequency-cutoff 20 \
	--approximant 'SPAtmplt:mtotal<4' 'SEOBNRv4_ROM:else' \
	--order -1 \
	--snr-threshold 5.5 \
	--cluster-method window \
	--cluster-window 1 \
	--cluster-function symmetric \
	--chisq-bins "0.72*get_freq('fSEOBNRv4Peak',params.mass1,params.mass2,params.spin1z,params.spin2z)**0.7" \
	--newsnr-threshold 5 \
	--filter-inj-only  \
	--injection-window 4.5 \
	--processing-scheme ${SCHEME} \
	--injection-filter-rejector-chirp-time-window 5 \
	--channel-name L1:GDS-CALIB_STRAIN \
	--gps-start-time 1126258462 \
	--gps-end-time 1126260462 \
	--trig-start-time 1126258700 \
	--trig-end-time 1126260000 \
	--output OUTPUTSEOB_${IDX}.hdf \
	--bank-file H1L1-SPLITBANK_0.hdf \
	--frame-files ${DATA_DIR}/L-L1_LOSC_4_V1-1126256640-4096.gwf \
	--channel-name L1:LOSC-STRAIN \
	&> log/test3_${IDX}.log &
done

wait

echo "Test 2 on ${phys_cores} cores"
echo
for IDX in $(seq 1 ${N_JOBS}); do
${PREFIX}/bin/pycbc_inspiral \
	--sgchisq-snr-threshold 6.0 \
	--sgchisq-locations "mtotal>40:20-30,20-45,20-60,20-75,20-90,20-105,20-120" \
	--pad-data 8 \
	--strain-high-pass 15 \
	--sample-rate 2048 \
	--segment-length 512 \
	--segment-start-pad 144 \
	--segment-end-pad 16 \
	--allow-zero-padding \
	--taper-data 1 \
	--psd-estimation median \
	--psd-segment-length 16 \
	--psd-segment-stride 8 \
	--psd-inverse-length 16 \
	--psd-num-segments 63 \
	--autogating-threshold 100 \
	--autogating-cluster 0.5 \
	--autogating-width 0.25 \
	--autogating-taper 0.25 \
	--autogating-pad 16 \
	--enable-bank-start-frequency  \
	--low-frequency-cutoff 20 \
	--approximant 'SPAtmplt:mtotal<50' 'SEOBNRv4_ROM:else' \
	--order -1 \
	--snr-threshold 5.5 \
	--cluster-method window \
	--cluster-window 1 \
	--cluster-function symmetric \
	--chisq-bins "0.72*get_freq('fSEOBNRv4Peak',params.mass1,params.mass2,params.spin1z,params.spin2z)**0.7" \
	--newsnr-threshold 5 \
	--filter-inj-only  \
	--injection-window 4.5 \
	--processing-scheme ${SCHEME} \
	--injection-filter-rejector-chirp-time-window 5 \
	--channel-name L1:GDS-CALIB_STRAIN \
	--gps-start-time 1126258462 \
	--gps-end-time 1126260462 \
	--trig-start-time 1126258700 \
	--trig-end-time 1126260000 \
	--output OUTPUTF2_${IDX}.hdf \
	--bank-file H1L1-SPLITBANK_0.hdf \
	--frame-files ${DATA_DIR}/L-L1_LOSC_4_V1-1126256640-4096.gwf \
	--channel-name L1:LOSC-STRAIN \
	&> log/test4_${IDX}.log &
done

wait

${PREFIX}/bin/python - <<EOF
from pathlib import Path
import glob
import h5py

for tag, outf in (
    ('OUTPUTSEOB', 'no_hp_perf_num_1.dat'),
    ('OUTPUTF2', 'no_hp_perf_num_2.dat'),
):
    filelist = list(Path('.').glob(f'{tag}_*.hdf'))
    assert len(filelist) == ${N_JOBS}

    perf_tot = 0
    for f in filelist:
        with h5py.File(f, 'r') as h5f:
            perf_tot += 1. / h5f['L1/search/run_time'][0]

    Path(outf).write_text(str(perf_tot))
EOF

popd

# -- waveform

echo "Running waveform benchmark"
echo

pushd ${RUN_DIR}/waveform_benchmark

# REMOVE SMALL FOR THE FULL BENCHMARK
${DATA_DIR}/TD-wf-bench/wf_bench.sh ${phys_cores}
echo
${DATA_DIR}/TD-wf-bench/wf_bench.sh ${log_cores}
echo

popd

# -- lalinference

echo "Running test on ${phys_cores} cores"
echo

pushd ${RUN_DIR}/lalinference_benchmark

# USE THIS SETTING WHEN DOING A PRODUCTION BENCHMARK
export NSTEPS=40000

# USE THIS SETTING IF WANTING TO TEST THE CODE IS FUNCTIONING
#export NSTEPS=200

TIMEFORMAT=%R

rm -rf TEST_${phys_cores}
mkdir TEST_${phys_cores}
cd TEST_${phys_cores}

for IDX in $(seq 1 ${phys_cores}); do
mkdir RUN_${IDX}
cd RUN_${IDX}
{ time ${PREFIX}/bin/lalinference_bench \
	--psdlength 1024 \
	--psdstart 441416535.0 \
	--seglen 32 \
	--srate 4096.0 \
	--trigtime 441417609 \
	--ifo H1 \
	--H1-channel H1:LDAS-STRAIN \
	--H1-cache LALSimAdLIGO \
	--dataseed 1324 \
	--chirpmass-max 6.170374 \
	--chirpmass-min 3.346569 \
	--q-min 0.125 \
	--comp-max 21.9986477179 \
	--comp-min 1.49140053129 \
	--disable-spin \
	--amporder 0 \
	--fref 100 \
	--inj ../../injections.xml \
	--event 0 \
	--H1-timeslide 0 \
	--trigtime 441417609 \
	--psdstart 441416535.0 \
	--tol 1.0 \
	--H1-flow 20.0 \
	--H1-fhigh 2047.96875 \
	--ntemps 8 \
	--np 8 \
	--nsteps 1 \
	--skip 100 \
	--approx IMRPhenomPv2pseudoFourPN \
	--outfile samples.hdf5  \
	--randomseed 1829391048 \
	--L1-flow 20.0  \
	--V1-timeslide 0 \
	--V1-cache LALSimAdVirgo  \
	--L1-channel L1:LDAS-STRAIN \
	--L1-fhigh 2047.96875 \
	--V1-channel V1:h_16384Hz \
	--V1-fhigh 2047.96875  \
	--V1-flow 20.0 \
	--L1-cache LALSimAdLIGO \
	--L1-timeslide 0 \
	--ifo V1 \
	--ifo L1 \
	--no-detector-frame \
	--Niter ${NSTEPS} &> logging; } 2> runtime &
cd ..
done
wait

${PREFIX}/bin/python - <<EOF
from pathlib import Path
import h5py

total_work = 0
for i in range(${phys_cores}):
    with open('RUN_{}/runtime'.format(i+1), 'r') as file:
        runtime = float(file.readline())
    total_work += 1./runtime

Path('../no_hp_benchmark.dat').write_text(str(total_work))
EOF

cd ..

echo "Running test on ${log_cores} cores"
echo
# This line is important! Without it some parts of the job can unexpectedly start using multiple threads!
TIMEFORMAT=%R

rm -rf TEST_${log_cores}
mkdir TEST_${log_cores}
cd TEST_${log_cores}

for IDX in $(seq 1 ${log_cores}); do
mkdir RUN_${IDX}
cd RUN_${IDX}
{ time \
${PREFIX}/bin/lalinference_bench \
	--psdlength 1024 \
	--psdstart 441416535.0 \
	--seglen 32 \
	--srate 4096.0 \
	--trigtime 441417609 \
	--ifo H1 \
	--H1-channel H1:LDAS-STRAIN \
	--H1-cache LALSimAdLIGO \
	--dataseed 1324 \
	--chirpmass-max 6.170374 \
	--chirpmass-min 3.346569 \
	--q-min 0.125 \
	--comp-max 21.9986477179 \
	--comp-min 1.49140053129 \
	--disable-spin \
	--amporder 0 \
	--fref 100 \
	--inj ../../injections.xml \
	--event 0 \
	--H1-timeslide 0 \
	--trigtime 441417609 \
	--psdstart 441416535.0 \
	--tol 1.0 \
	--H1-flow 20.0 \
	--H1-fhigh 2047.96875 \
	--ntemps 8 \
	--np 8 \
	--nsteps 1 \
	--skip 100 \
	--approx IMRPhenomPv2pseudoFourPN \
	--outfile samples.hdf5  \
	--randomseed 1829391048 \
	--L1-flow 20.0  \
	--V1-timeslide 0 \
	--V1-cache LALSimAdVirgo  \
	--L1-channel L1:LDAS-STRAIN \
	--L1-fhigh 2047.96875 \
	--V1-channel V1:h_16384Hz \
	--V1-fhigh 2047.96875  \
	--V1-flow 20.0 \
	--L1-cache LALSimAdLIGO \
	--L1-timeslide 0 \
	--ifo V1 \
	--ifo L1 \
	--no-detector-frame \
	--Niter ${NSTEPS} &> logging;
} 2> runtime &
cd ..
done
wait

${PREFIX}/bin/python - <<EOF
from pathlib import Path
import h5py

total_work = 0
for i in range(${log_cores}):
    with open('RUN_{}/runtime'.format(i+1), 'r') as file:
        runtime = float(file.readline())
    total_work += 1./runtime

Path('../w_hp_benchmark.dat').write_text(str(total_work))
EOF

popd

# -- summary ------------------------------------------------------------------

echo "SUMMARIZING"
echo
cd ${RUN_DIR}

${PREFIX}/bin/python - <<EOF
import numpy

# load PyCBC benchmarks
pycbc_1A = numpy.loadtxt('pycbc_benchmark/no_hp_perf_num_1.dat')[()]
pycbc_1B = numpy.loadtxt('pycbc_benchmark/no_hp_perf_num_2.dat')[()]
pycbc_2A = numpy.loadtxt('pycbc_benchmark/w_hp_perf_num_1.dat')[()]
pycbc_2B = numpy.loadtxt('pycbc_benchmark/w_hp_perf_num_2.dat')[()]
for val in [pycbc_1A, pycbc_1B, pycbc_2A, pycbc_2B]:
    assert type(val) == numpy.float64
    assert val > 0.

# load waveform benchmarks
waveform_1 = numpy.loadtxt("waveform_benchmark/benchmark_${phys_cores}.dat")[()]
waveform_2 = numpy.loadtxt("waveform_benchmark/benchmark_${log_cores}.dat")[()]
for val in [waveform_1, waveform_2]:
    assert type(val) == numpy.float64
    assert val > 0.

# load lalinference benchmarks
lalinference_1 = numpy.loadtxt('lalinference_benchmark/no_hp_benchmark.dat')[()]
lalinference_2 = numpy.loadtxt('lalinference_benchmark/w_hp_benchmark.dat')[()]
for val in [lalinference_1, lalinference_2]:
    assert type(val) == numpy.float64
    assert val > 0.

print("\nFINAL BENCHMARK FIGURES FOLLOW\n")

norm_pycbc = 0.022060000000000003 / 32.
norm_waveform = 0.000181422351233672 / 32.
norm_lalinf = 0.00639 / 32.

print("FOM_pycbc", max(pycbc_1A+pycbc_1B, pycbc_2A+pycbc_2B) / norm_pycbc)
print("FOM_waveform", max(1./waveform_1, 1./waveform_2) / norm_waveform)
print("FOM_lalinference", lalinference_1 / norm_lalinf)

print("\nAlso writing figures to RESULTS.dat")
with open('RESULTS.dat', 'w') as fp:
    print("FOM_pycbc", max(pycbc_1A+pycbc_1B, pycbc_2A+pycbc_2B) / norm_pycbc, file=fp)
    print("FOM_waveform", max(1./waveform_1, 1./waveform_2) / norm_waveform, file=fp)
    print("FOM_lalinference", lalinference_1 / norm_lalinf, file=fp)
EOF
